{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29611969",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5636b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b33aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8374fe",
   "metadata": {},
   "source": [
    "# Introducing the DataFrames API\n",
    "In Spark, a DataFrame object consists of [Row](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html) objects and [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) objects. Concretely, each row of a Spark DataFrame  is an instance of the ```pyspark.sql.Row``` while each column is an instance of the ```pyspark.sql.Column``` class. We will look at  each of these classes in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260a945",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "1. From Python objects\n",
    "2. External data sources\n",
    "3. Other Spark objects\n",
    "\n",
    "### Schemas\n",
    "Also, when creating DataFrames, you have the option to use a schema or not. A schema in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data from an external data source. When a schema is not used, Spark has to infer the data type which can slow your application if you have a massive  dataset. Although schemas are more of DBMS language but they offer several advantages when dealing with large datasets:\n",
    "- Spark doesnt have to infer data types, so you get speed benefits.\n",
    "- Without a schema, Spark creates a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming. As such, defining a schema will avoid this.\n",
    "- You can detect errors early if data doesn’t match the schema.\n",
    "#### Defining Schemas\n",
    "- Programmatically using Spark DataTypes \n",
    "- Using Data Definition Language (DDLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d225",
   "metadata": {},
   "source": [
    "### Spark DataFrame from Python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema using Spark DataTypes\n",
    "schema = StructType([StructField(\"author_name\", StringType(), False),\n",
    "      StructField(\"book_title\", StringType(), False),\n",
    "      StructField(\"num_pages\", IntegerType(), False)])\n",
    "\n",
    "# Define Schema using DDL\n",
    "schema = \"author_name STRING, book_title STRING, num_pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data using DDL\n",
    "schema = \"author_name STRING, book_title STRING, num_pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple statistic data\n",
    "# in real life, we can get alot data in the o=form of Python objects and want to create SparkDataFrames\n",
    "# for instance, data being downloaded from websites\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "           [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "           [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "    [\"twitter\", \"FB\"]],\n",
    "           [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "    [\"twitter\", \"LinkedIn\"]]\n",
    "          ]\n",
    "\n",
    "# Create a SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "spark=SparkSession.builder.appName(\"intro\").master(\"local[*]\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "sdf = spark.createDataFrame(data, %colorsnames=[])\n",
    "# Show the DataFrame; it should reflect our table above blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bee41f",
   "metadata": {},
   "source": [
    "### EXERCISE-1: READ CSV WITH SCHEMA\n",
    "1. Use Spark documentation on how to read from file with a define schema. \n",
    "Note, the schema is what we arleady defined above. The data above has been saved as ```blog_simple_dataset.csv```. Read it as a Spark DataFrame with schema. Answer this question in the next cell.\n",
    "2. Define schema for the ```activity_raw_data.csv``` use string for the datetime column\n",
    "3. Load the dataset with and without schema using the functions defined below. Compare the loading times. Answer this question by completing the functions defined below and calling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7474b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\", nrows=1000)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556d4e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def timefn(fn):\n",
    "    \"\"\"\n",
    "    Function for recording running time of a function\n",
    "    \"\"\"\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c823db",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_with_schema(large_csv):\n",
    "    # define the schema here using DDL\n",
    "    # you can load part of the file with pandas (just a few rows)\n",
    "    # to remind yourself of the data types\n",
    "    schema = \"`SID` INT, `ACTIVITY_ID` INT, `Last` STRING, `ACTIVITY_TIME` STRING,`STATUS` STRING\"\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"ReadWithChema\").getOrCreate()\n",
    "    # Now read the data \n",
    "    sdf = spark.read.schema(schema).csv(large_csv)\n",
    "    \n",
    "    print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd849bb",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_without_schema(large_csv):\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "    sdf = spark.read.csv(large_csv, header=True)\n",
    "    print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2de6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_with_schema(\"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22962c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_without_schema(\"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82841192",
   "metadata": {},
   "source": [
    "### Spark DataFrame from external data sources\n",
    "The most common way (which we have already seen) is to load data from exteernal data sources and \n",
    "Spark supports numerous data stores. Spark reads data  through the ```DataFrameReaderobject```. Please look at the documeentation [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html) to see all data sources that the Spark  ```DataFrameReaderobject``` supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"SparkConnectors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002acff4",
   "metadata": {},
   "source": [
    "## Columns and Expressions in  DataFrames\n",
    "In Spark DataFrames, columns behave like pandas DataFrames in several ways but they also behave different. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. \n",
    "- [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) is the name of the object, which has many import methods such as describe  while [col()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.col.html) is a standard built-in function that returns a Column.\n",
    "\n",
    "We need to use the col() and expr() function available in pyspark,sql.functions() for many operations such as:\n",
    "- Add, rename columns\n",
    "- Subset data based on columns\n",
    "- Access columns to compute stats on them\n",
    "-  Access columns to compute operations on them such as sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33802",
   "metadata": {},
   "source": [
    "### Add a new column using expr and col\n",
    "In order to add a new column in a Spark DataFrame, we use the ```DataFrame.withColumn(new_col_name, expression_to_compute_new_col)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67d84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29613aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 11:55:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "csv_fpath = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\"\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "sdf = spark.read.csv(csv_fpath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use expr\n",
    "sdf2 = sdf.withColumn(\"new_col\", (expr(\"ACTIVITY_ID > 10000\")))\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147bcc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+------+-------+\n",
      "|SID|ACTIVITY_ID|       ACTIVITY_TIME|STATUS|new_col|\n",
      "+---+-----------+--------------------+------+-------+\n",
      "|584|       1291|13-APR-15 10.33.4...|     S|  false|\n",
      "|584|       1286|13-APR-15 10.33.4...|     S|  false|\n",
      "|584|       1285|13-APR-15 10.33.4...|     S|  false|\n",
      "|584|       1284|13-APR-15 10.33.4...|     S|  false|\n",
      "|584|       1288|13-APR-15 10.33.4...|     S|  false|\n",
      "|584|       1293|13-APR-15 10.33.4...|     S|  false|\n",
      "|344|         10|13-APR-15 10.33.3...|     N|  false|\n",
      "|344|         10|13-APR-15 10.33.3...|     R|  false|\n",
      "|344|         10|13-APR-15 10.33.3...|     N|  false|\n",
      "|584|       1269|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1268|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1267|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1266|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1265|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1264|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1263|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1262|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1261|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1260|13-APR-15 10.33.2...|     S|  false|\n",
      "|584|       1259|13-APR-15 10.33.2...|     S|  false|\n",
      "+---+-----------+--------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the col function which I prefer over the expr col(\"Hits\")\n",
    "sdf2 = sdf.withColumn(\"new_col\", col(\"ACTIVITY_ID\") > 10000)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f41ff",
   "metadata": {},
   "source": [
    "### Subset data  based on a few columns\n",
    "In order to access a single or multiple columns, we use the ```select()``` function on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = sdf.select('ACTIVITY_TIME', 'STATUS')\n",
    "sdf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1439c",
   "metadata": {},
   "source": [
    "**EXERCISE-2:**\n",
    "\n",
    "1. Check  if these statements: df.select(expr(\"ACTIVITY_TIME\")).show(2), df.select(col(\"ACTIVITY_TIME\")).show(2)\n",
    "and df.select(\"ACTIVITY_TIME\").show(2) will provide  the same output. Replace df with name of your Spark DataFrame.\n",
    "\n",
    "2. Create a new DataFrame using expr to get only those rows where STATUS is \"S\"\n",
    "Note that expr() just perfoms the operation, it doesnt filter our the rows which evaluate to false.\n",
    "2. Sort DataFrame: use the col function to sort the DataFrame on \"SID\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85821fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.sort(col(\"SID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13d560",
   "metadata": {},
   "source": [
    "# SPARK DATAFRAME = ROWS + COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6d3078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c356c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = sdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c695de51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_row = first_row[0]\n",
    "type(spark_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba3e0c",
   "metadata": {},
   "source": [
    "### Rows\n",
    "A row in Spark is a generic Row object, containing one or more columns. Each column may be of the same data type (e.g., integer or string), or they can have different types (integer, string, map, array, etc.). Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row the same way we instantiate any object. Consequently, you can collect Row objects in a list and create a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab276eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "row = Row(name=\"Alice\", age=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae4bb7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         name|state|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(name=\"Matei Zaharia\", state=\"CA\"), Row(name=\"Reynold Xin\", state=\"CA\")]\n",
    "spark_df_from_rows = spark.createDataFrame(rows)\n",
    "spark_df_from_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057fc79",
   "metadata": {},
   "source": [
    "**EXERCISE-3:** Creating a Spark DataFrame with Rows. Please complete the function below and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8616b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def convert_json_to_spark_with_rows(json_file):\n",
    "    # create  a list to hold all Row objects\n",
    "    rows = YOUR CODE\n",
    "    for idx, row in df.iterrows():\n",
    "        # get lon and lat from the coord column using indexing, dict key access\n",
    "        x = row['coord']['lon']       \n",
    "        y = row['coord']['lat']\n",
    "        # create the Row object here \n",
    "        srow = YOUR CODE\n",
    "        \n",
    "        # append this row object to the list\n",
    "        YOUR CODE\n",
    "    \n",
    "    # When creating Spark DataFrame this way, its better to use schema to avoid troubles\n",
    "    # create a schema for this data here, use DOUBLE as data type for lon and lat\n",
    "    schema = YOUR CODE\n",
    "    \n",
    "    # use spark.createDataFrame() here\n",
    "    # if yiu get errors, use the option verifySchema=False\n",
    "    spark_df = YOUR CODE\n",
    "    \n",
    "    # use show() statement to show the DataFrame\n",
    "    # use show() with print to ensure we see the outputs\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beabae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = \"../data/city.list.json\"\n",
    "#convert_json_to_spark_with_rows(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa540555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707860</td>\n",
       "      <td>Hurzuf</td>\n",
       "      <td>UA</td>\n",
       "      <td>{'lon': 34.283333, 'lat': 44.549999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519188</td>\n",
       "      <td>Novinki</td>\n",
       "      <td>RU</td>\n",
       "      <td>{'lon': 37.666668, 'lat': 55.683334}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283378</td>\n",
       "      <td>Gorkhā</td>\n",
       "      <td>NP</td>\n",
       "      <td>{'lon': 84.633331, 'lat': 28}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1270260</td>\n",
       "      <td>State of Haryāna</td>\n",
       "      <td>IN</td>\n",
       "      <td>{'lon': 76, 'lat': 29}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>708546</td>\n",
       "      <td>Holubynka</td>\n",
       "      <td>UA</td>\n",
       "      <td>{'lon': 33.900002, 'lat': 44.599998}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id              name country                                 coord\n",
       "0   707860            Hurzuf      UA  {'lon': 34.283333, 'lat': 44.549999}\n",
       "1   519188           Novinki      RU  {'lon': 37.666668, 'lat': 55.683334}\n",
       "2  1283378            Gorkhā      NP         {'lon': 84.633331, 'lat': 28}\n",
       "3  1270260  State of Haryāna      IN                {'lon': 76, 'lat': 29}\n",
       "4   708546         Holubynka      UA  {'lon': 33.900002, 'lat': 44.599998}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(jsonfile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d689c1",
   "metadata": {},
   "source": [
    "# Common DataFrames Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"`id` INT, `name` STRING, `country` STRING, `lon` DOUBLE,`lat` DOUBLE\"\n",
    "spark_df = spark.createDataFrame(rows, schema=schema, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ceff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b54c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lat.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fpath = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\"\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "sdf = spark.read.csv(csv_fpath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = sdf.select('ACTIVITY_TIME', 'STATUS')\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789019d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = sdf.select('ACTIVITY_TIME', 'STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").config('spark.executor.memory', '1g').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fpath = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\"\n",
    "sdf = spark.read.csv(csv_fpath, header=True)\n",
    "sdf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff91f4f",
   "metadata": {},
   "source": [
    "# How Spark SQL executes computations\n",
    "Lets use the expllain() function on a DataFrame to see how the Spark SQL engine plans its execution. For complicated and time consuming jobs, understanding this logical plan before running the job is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ddc44e",
   "metadata": {},
   "source": [
    "### CHANGING CONFIGS USING SPARK-ENV.SH AND SPARK-DEFAULTS.CONF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54e74ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 09:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/20 09:29:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                    .appName(\"DataFrameFromPythonObj\")\\\n",
    "                    .config('spark.executor.memory', '2g')\\\n",
    "                    .config('spark.driver.memory', '4g')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478ffd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(CSV_PATH, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b445b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_by_status = sdf.groupBy('STATUS').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fdec8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['STATUS], [unresolvedalias('STATUS, None), count(1) AS count#42L]\n",
      "+- Relation[SID#16,ACTIVITY_ID#17,ACTIVITY_TIME#18,STATUS#19] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "STATUS: string, count: bigint\n",
      "Aggregate [STATUS#19], [STATUS#19, count(1) AS count#42L]\n",
      "+- Relation[SID#16,ACTIVITY_ID#17,ACTIVITY_TIME#18,STATUS#19] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [STATUS#19], [STATUS#19, count(1) AS count#42L]\n",
      "+- Project [STATUS#19]\n",
      "   +- Relation[SID#16,ACTIVITY_ID#17,ACTIVITY_TIME#18,STATUS#19] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[STATUS#19], functions=[count(1)], output=[STATUS#19, count#42L])\n",
      "+- Exchange hashpartitioning(STATUS#19, 200), true, [id=#53]\n",
      "   +- *(1) HashAggregate(keys=[STATUS#19], functions=[partial_count(1)], output=[STATUS#19, count#46L])\n",
      "      +- FileScan csv [STATUS#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<STATUS:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_by_status.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4509e7b",
   "metadata": {},
   "source": [
    "# The SQL API: interact with a CSV file read as DataFrame with SQL commands\n",
    "As mentioned, Spark allows you to read (ee.g., a CSV file) in data as DataFrame but you can interact with it using good old SQL commands. The following steps are required in order to \n",
    "1. **Create a DataFrame as required:**. In our case, we will read from external source.\n",
    "2. **Create a table  view:**. Views are a special version of tables in SQL. They provide a virtual table environment for various complex operations. You can select data from multiple tables, or you can select specific data based on certain criteria in views. It does not hold the actual data; it holds only the definition of the view in the data dictionary (you will learn more about this in the Database course).\n",
    "\n",
    "Once you have a temporary view, you can issue SQL queries using Spark SQL. These queries are no different from those you might issue against a SQL table in, say, a MySQL or PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-1: Read in data as DataFrame\n",
    "sdf = spark.read.csv(CSV_PATH, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633fa966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2: Create a table view\n",
    "sdf.createOrReplaceTempView('hello_SQL_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4742a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+------+\n",
      "|SID|ACTIVITY_ID|       ACTIVITY_TIME|STATUS|\n",
      "+---+-----------+--------------------+------+\n",
      "|584|       1291|13-APR-15 10.33.4...|     S|\n",
      "|584|       1286|13-APR-15 10.33.4...|     S|\n",
      "|584|       1285|13-APR-15 10.33.4...|     S|\n",
      "|584|       1284|13-APR-15 10.33.4...|     S|\n",
      "|584|       1288|13-APR-15 10.33.4...|     S|\n",
      "|584|       1293|13-APR-15 10.33.4...|     S|\n",
      "|344|         10|13-APR-15 10.33.3...|     N|\n",
      "|344|         10|13-APR-15 10.33.3...|     R|\n",
      "|344|         10|13-APR-15 10.33.3...|     N|\n",
      "|584|       1269|13-APR-15 10.33.2...|     S|\n",
      "+---+-----------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3a02ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================================> (61 + 2) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|STATUS|       ACTIVITY_TIME|\n",
      "+------+--------------------+\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "|     S|31-OCT-15 12.48.3...|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# issue the SQL query\n",
    "spark.sql(\"\"\"SELECT STATUS, ACTIVITY_TIME FROM hello_SQL_tbl WHERE STATUS == 'S' ORDER BY ACTIVITY_TIME DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed598ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH, nrows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9c75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean       390.536400\n",
       "std        204.507279\n",
       "min          1.000000\n",
       "25%        236.000000\n",
       "50%        471.000000\n",
       "75%        584.000000\n",
       "max        814.000000\n",
       "Name: SID, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.SID.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea819f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
